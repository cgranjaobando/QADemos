{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NDCG & MAP walkthrough\n",
    "Synthetic Swag Labs search evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "judgments = pd.read_csv(Path('data/judgments.csv'))\n",
    "runs = pd.read_csv(Path('data/runs.csv'))\n",
    "judgments.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def dcg(relevances):\n",
    "    return sum(rel / math.log2(idx + 2) for idx, rel in enumerate(relevances))\n",
    "\n",
    "def ndcg_at_k(jdf, rdf, k):\n",
    "    grouped_rel = jdf.groupby('query_id')\n",
    "    grouped_run = rdf.groupby('query_id')\n",
    "    scores = []\n",
    "    for qid, rel_group in grouped_rel:\n",
    "        gains = rel_group.set_index('doc_id')['grade']\n",
    "        run = grouped_run.get_group(qid).sort_values('rank').head(k)\n",
    "        retrieved = [int(gains.get(doc_id, 0)) for doc_id in run['doc_id']]\n",
    "        ideal = sorted(gains.tolist(), reverse=True)[:k]\n",
    "        denom = dcg(ideal)\n",
    "        scores.append(dcg(retrieved) / denom if denom else 0.0)\n",
    "    return sum(scores) / len(scores)\n",
    "\n",
    "def average_precision(relevances):\n",
    "    hits = 0\n",
    "    precision_sum = 0.0\n",
    "    for idx, rel in enumerate(relevances, start=1):\n",
    "        if rel > 0:\n",
    "            hits += 1\n",
    "            precision_sum += hits / idx\n",
    "    return precision_sum / hits if hits else 0.0\n",
    "\n",
    "def mean_average_precision(jdf, rdf, k=None):\n",
    "    grouped_rel = jdf.groupby('query_id')\n",
    "    grouped_run = rdf.groupby('query_id')\n",
    "    scores = []\n",
    "    for qid, rel_group in grouped_rel:\n",
    "        gains = rel_group.set_index('doc_id')['grade']\n",
    "        run = grouped_run.get_group(qid).sort_values('rank')\n",
    "        if k:\n",
    "            run = run.head(k)\n",
    "        relevances = [int(gains.get(doc_id, 0)) for doc_id in run['doc_id']]\n",
    "        scores.append(average_precision(relevances))\n",
    "    return sum(scores) / len(scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = {\n",
    "    'ndcg@3': ndcg_at_k(judgments, runs, 3),\n",
    "    'ndcg@5': ndcg_at_k(judgments, runs, 5),\n",
    "    'map@3': mean_average_precision(judgments, runs, 3),\n",
    "    'map@5': mean_average_precision(judgments, runs, 5),\n",
    "}\n",
    "pd.DataFrame([summary])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
